{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 7: Deploy de sistema de recomendación con Watson\n",
    "\n",
    "En este proyecto llevaremos a cabo la puesta en producción del modelo entrenado en el proyecto 5. Es decir, lo subirmos la nube de IBM y utilizando llamados a la API de Watson tendremos acceso a él para realizar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sklearn\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "moviedir = r'./dataset_movies/movie_reviews' \n",
    "movie_reviews = load_files(moviedir, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    movie_reviews.data, movie_reviews.target, test_size = 0.20, stratify=movie_reviews.target, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "eclf = joblib.load('sentiment.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Cargá** la biblioteca `WatsonMachineLearningAPIClient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade watson-machine-learning-client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 uninstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install scikit-learn==0.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Creá** variable con las credenciales que necesita `Watson`. Ellas son: `url, access_key, username, password e instance_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "NatLearUnder = {\n",
    "  \"apikey\": \"RqzBP7gaBbeFX2v4gAlUZTw2eBB4w1H9quVVIHl14uRC\",\n",
    "  \"iam_apikey_description\": \"Auto-generated for key 8655e708-3ee9-4138-a0e7-82a092cc9a9e\",\n",
    "  \"iam_apikey_name\": \"Auto-generated service credentials\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Manager\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/62c6cea1393d4576b56fdd7641e7d720::serviceid:ServiceId-50fce8b9-44bb-4fce-a4a6-aba1425edc1a\",\n",
    "  \"url\": \"https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/cf5cd667-3142-4d27-81fc-39e37a150e2a\"\n",
    "}\n",
    "\n",
    "Watsonml = {\n",
    "  \"apikey\": \"w2NAeiIZQo4rvz6PUsAklPaweCEqiSPj2cOKbu9PAKIK\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/62c6cea1393d4576b56fdd7641e7d720:c081364d-d603-4888-9337-272d3117a955::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-0457564c-a895-4da7-95e1-8c955d0b4df6\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/62c6cea1393d4576b56fdd7641e7d720::serviceid:ServiceId-50fce8b9-44bb-4fce-a4a6-aba1425edc1a\",\n",
    "  \"instance_id\": \"c081364d-d603-4888-9337-272d3117a955\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#NaturalLUnder = NaturalLanguageUnderstandingV1(\n",
    "#    version='2018-11-16',\n",
    "#    iam_api_key='RqzBP7gaBbeFX2v4gAlUZTw2eBB4w1H9quVVIHl14uRC',\n",
    "#    url='https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/cf5cd667-3142-4d27-81fc-39e37a150e2a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Declará** la variable `client` y guardá en ella al objeto `WatsonMachineLearningAPIClient` con las credenciales como parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(Watsonml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Creá** una variable que guarde las propiedades del modelo. Datos del autor y nombre del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_properties = { client.repository.ModelMetaNames.AUTHOR_NAME       : \"Ricardo Naranjo\", \n",
    "                     client.repository.ModelMetaNames.NAME              : \"CritiKon\",\n",
    "                     client.repository.ModelMetaNames.FRAMEWORK_NAME    : \"scikit-learn\",\n",
    "                     client.repository.ModelMetaNames.FRAMEWORK_VERSION : \"0.19\",\n",
    "                     client.repository.ModelMetaNames.RUNTIME_NAME      : 'python',\n",
    "                     client.repository.ModelMetaNames.RUNTIME_VERSION   : '3.6'\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Hacé** un pipeline que contenga como primer paso a un `TfidfVectorizer` y como segundo paso, al mejor modelo que hayas obtenido en el proyecto 5. **Entrená** con este pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar(lista, lista_lista, stopers, decodeit):\n",
    "    for i in lista:\n",
    "        if (decodeit):\n",
    "            letters = i.decode(\"utf-8\")\n",
    "            letters_only = re.sub(\"\\n\", \"\", letters)\n",
    "        else: \n",
    "            letters_only = re.sub(\"\\n\", \"\", i)\n",
    "        tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "        letteres = tokenizer.tokenize(letters_only)\n",
    "        leters = \" \".join(letteres)\n",
    "        word_tokens = nltk.word_tokenize(leters)\n",
    "        texto = [word for word in word_tokens if word not in stopers]\n",
    "        leters = \" \".join(texto).lower()\n",
    "        #print(leters.lower())\n",
    "        lista_lista.append(leters)\n",
    "    return lista_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( movie_reviews.data, movie_reviews.target, test_size = 0.20, stratify=movie_reviews.target, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "listik = []\n",
    "textlist_train = preprocesar(X_train, listik, stops, True)\n",
    "text_clf= Pipeline([('tfidf', TfidfVectorizer()), ('ada_clf', AdaBoostClassifier(n_estimators=100))])\n",
    "text_clf.fit(textlist_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Subí** al modelo a IBM Cloud usando `client.repository.store_model` con los parámetros correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = client.repository.store_model(text_clf, meta_props=model_properties, training_data=None)\n",
    "\n",
    "#published_model = client.repository.store_model(model=pipeline, meta_props=model_props, training_data=X_train, training_target=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_uid = client.repository.get_model_uid(model_details)\n",
    "model_details = client.repository.get_details(published_model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Obtené** el `uid` del modelo y guardalo en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  --------  ------------------------  -----------------\n",
      "GUID                                  NAME      CREATED                   FRAMEWORK\n",
      "b9e260ef-e9c1-475a-ae3d-057d5fd20704  CritiKon  2020-04-15T02:27:23.317Z  scikit-learn-0.19\n",
      "f226ecb5-154d-4b0d-a2d6-58bd3f5b8aeb  CritiKon  2020-04-15T02:27:06.509Z  scikit-learn-0.19\n",
      "e5f02a3f-785d-4112-900e-b6096cfb4311  CritiKon  2020-04-15T02:12:53.504Z  scikit-learn-0.19\n",
      "------------------------------------  --------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "models_details = client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Cargá** el modelo basado en su `uid` y utilizalo para realizar la predicción sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = client.repository.load(published_model_uid)\n",
    "test_predictions = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Mostrar** el `classification_report` obtenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc score:  0.77\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.76      0.77       200\n",
      "          1       0.76      0.78      0.77       200\n",
      "\n",
      "avg / total       0.77      0.77      0.77       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"roc_auc score: \", roc_auc_score(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
